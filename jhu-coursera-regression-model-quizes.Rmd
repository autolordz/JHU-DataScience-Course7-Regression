---
title: "JHU Coursera Regression Model Quizes"
author: "Autoz"
date: '`r format(Sys.Date())`'
editor_options:
  chunk_output_type: console
slug: jhu-coursera-regression-model-quizes
tags: DataScience
categories: R
output: 
    github_document:
        toc: yes
        toc_depth: 2
  # html_document:
  #   df_print: tibble
  #   highlight: textmate
  #   keep_md: yes
  #   number_sections: yes
  #   theme: readable
  #   toc: yes
  #   toc_depth: 2
---

# JHU DataScience Specialization/Cousers Reproducible Data/Week1-4/Regression Model Quizes

# Quizes

主要练习手工计算回归模型的基础方法

```{r echo=F,warning=F,message=F}
knitr::opts_chunk$set(cache =T, autodep =T)
pacman::p_load(tidyverse,data.table,ggplot2,ggpubr,pander)
```

## Week 2

### Quiz 1

手算均值
```{r quiz21}
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
mu.y <- sum(w * x) / sum(w)
sprintf("mean of y is : %f",mu.y)
```

### Quiz 2 

线性回归
```{r quiz22}
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
pander(lm(y~x)) #THROUGH THE ORIGIN
pander(lm(y~x-1)) #去除截距
```

### Quiz 3

mtcars 回归系数
```{r quiz23 ,echo=F}
data(mtcars)
pander(coef(lm(mpg~wt,mtcars)))
```

### Quiz 4

练习求b1
$$
Cor(Y,X) = 0.5 \qquad
Sd(Y) = 1  \qquad Sd(X) = 0.5 \\ 
\beta_1 = Cor(Y,X) * \frac{Sd(Y)}{Sd(X)}
$$

```{r quiz24}
B1 = 0.5 * 1 / 0.5
```

### Quiz 5

```{r quiz25}
corr <- .4; emean <- 0; varr1 <- 1
varr2 <- 1; b0 <- 0; x <- 1.5
b1 <- corr * sqrt(varr1) / sqrt(varr2)
(y <- b0 + b1 * x)
```

### Quiz 6
```{r quiz26}
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
(x - mean(x)) / sd(x) # Choose No.1
```

### Quiz 7

```{r quiz27}
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y~x)
```

### Quiz 8

> It must be identically 0.

### Quiz 9

```{r quiz29}
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
mean(x)
```

### Quiz 10

$$
\beta_1=Cor(Y,X)*Sd(Y)/Sd(X) \\ 
Y_1=Cor(Y,X)*Sd(X)/Sd(Y) \\
\beta_1/Y_1= Sd(Y)^2/Sd(X)^2 = Var(Y)/Var(X)
$$

## Week 3

### Quiz 1

求系数

```{r quiz31}
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lm(y~x)
pander(summary(fit)$coefficients)
```

### Quiz 2

总平方和与回归与残差关系

$$
y_i = \beta_0 + \beta_1 x_i + e_i \\
\hat y_i = \beta_0 + \beta_1 x_i + e_i \\
e_i = y_i - \hat y_i \\
SS_{total} = \|y_i-\bar y \mathbf{1}\|^2 = \sum_{i=1}^n (y_i-\bar y)^2 \\ 
= \| \hat y_i-\bar y \mathbf{1}\|^2 + \|\hat \epsilon\|  
= \sum_{i=1}^n (\hat y_i-\bar y)^2 + \sum_{i=1}^n (y_i-\hat y)^2 \\ 
= SS_{regression} + SS_{residual} \\ \mathbf{1} = (1,1,\ldots,1)^T
$$

平方和与残差

$$
SS_x = \sum_{i=1}^n {(x_i - \bar x)^2} \\
e_i = y_i - (\beta_1x_i + \beta_0) \\
\hat \sigma^2 = \frac{1}{n-2} \sum_{i=1}^n e_i^2 = \frac{1}{n-2} SS_{residual} \sim \chi_{(n-2)}^2
$$

```{r quiz321}
# init
n <- length(x)
beta1 <- cor(y, x) * sd(y) / sd(x)
beta0 <- mean(y) - beta1 * mean(x)
ssx <- sum((x - mean(x))^2)
e <- y - (beta1 * x + beta0)
sigma <- sqrt(sum(e^2) / (n-2))
```

方差与系数方差关系

$$
\sigma^2_{\hat\beta_1}=Var(\hat\beta_1)=\frac{\sigma^2}{SS_x}\\
\sigma^2_{\hat\beta_0}=Var(\hat\beta_0)=(\frac{1}{n}+\frac{\bar X^2}{SS_x})\sigma^2 \\
$$

$$
R^2 = \frac{SS_{regression}}{SS_{total}} = \frac{\sum_{i=1}^n (\hat y_i-\bar y)^2}{\sum_{i=1}^n (y_i-\bar y)^2}
$$
t分布与方差
$$
\frac{\hat\beta_j-\beta_j}{\sigma_{\hat\beta_j}} \sim t_{\beta_j}(n-p)
$$

```{r quiz322}
# 计算 Beta1
seBeta0 <- sqrt(1/n + mean(x)^2/ssx)*sigma
seBeta1 <- sigma / sqrt(ssx)
tBeta0 <- beta0 / seBeta0
tBeta1 <- beta1 / seBeta1
pBeta0 <- 2 * pt(tBeta0, df = n-2, lower.tail = FALSE)
pBeta1 <- 2 * pt(tBeta1, df = n-2, lower.tail = FALSE)
```

```{r quiz323,echo=F}
# 手工构造系数表
coefTable <- rbind(c(beta0, seBeta0, tBeta0, pBeta0),
                   c(beta1, seBeta1, tBeta1, pBeta1))
colnames(coefTable) <- c("Estimate", "Std. Error", "t value", "P(>|t|)")
rownames(coefTable) <- c("(Intercept)", "x")
pander(coefTable)
```

### Quiz 3

Dataset: mtcars

```{r quiz331}
# 构造模型
fit <- lm(mpg~wt,mtcars)
pander(summary(fit))
sumCoef <- summary(fit)$coefficients
b0 <- sumCoef[1,1]
b1 <- sumCoef[2,1]
seb0 <- sumCoef[1, 2]
seb1 <- sumCoef[2, 2]
b0i <- b0 + c(-1, 1) * qt(.9, df = fit$df) * seb0
b1i <- b1 + c(-1, 1) * qt(.9, df = fit$df) * seb1 
x0 <- mean(mtcars$wt)
```

```{r quiz332,echo=F}
# 模型预测x0的y0
A = predict.lm(fit,newdata = data.frame(wt=x0),interval = ("confidence"))
B = predict.lm(fit,newdata = data.frame(wt=x0),interval = ("prediction"))
y0 <- b0 + b1 * x0 # 定点
yi <- b0i + b1i * x0
C = c(y0,yi)
A <- A %>% rbind(B,C) %>% set_rownames(c("confidence","prediction","manual")) 
pander(A)
```

```{r quiz333,echo=F}
# Predict New Data Plot
new <- data.frame(wt=rnorm(100,mean=x0))
p1 <- data.frame(predict(fit, new, interval = "confidence"))
p2 <- data.frame(predict(fit, new, interval = "prediction"))
p1$interval = "confidence"
p2$interval = "prediction"
p1$wt = new$wt
p2$wt = new$wt
dat = rbind(p1, p2)
names(dat)[1] = "mpg"
dat <- as.tbl(dat)
dat %>%
    ggplot(aes(x = wt, y = mpg))+
    geom_ribbon(aes(ymin = lwr, ymax = upr, fill = interval), alpha = 0.2)+
    geom_line()+
    geom_point(data = mtcars,aes(x = wt, y = mpg), size = 3)
```

### Quiz 4

> The estimated expected change in mpg per 1,000 lb increase in weight.

### Quiz 5

```{r quiz35}
pander(predict(fit,newdata = data.frame(wt=3000/1000),interval = ("prediction")))
```

### Quiz 6

```{r quiz361,echo=F}
# 构造模型
y = mtcars$mpg
x = mtcars$wt/2
fit2<-lm(y~x)
sumCoef <- summary(fit2)$coefficients
b1 <- sumCoef[2,1]
b0 <- sumCoef[1,1]
seb1 <- sumCoef[2, 2]
seb0 <- sumCoef[1, 2]
x0 <- mean(mtcars$wt/2)
y0 <- b0 + b1 * x0 # 定点
b0i <- b0 + c(-1, 1) * qt(.975, df = fit2$df) * seb0
b1i <- b1 + c(-1, 1) * qt(.975, df = fit2$df) * seb1 
yi <- b0i + b1i * x0
```

```{r quiz362,echo=F}
# 模型预测x0的y0
P <- data.frame(y=c(y0,yi))
P <- transpose(P)
names(P) <- c("fit","lwr","upr")
pander(P)

# predict fun 预测
DF = data.frame(x=mean(x))
pander(colMeans(predict(fit2,newdata = DF,interval = ("confidence"))))
pander(colMeans(predict(fit2,newdata = DF,interval = ("prediction"))))
```

### Quiz 7

```{r}
fit3<-lm(mpg~I(wt/100),mtcars)
pander(summary(fit3)$coefficients)
#It would get multiplied by 100.
```

### Quiz 8

>The new intercept would be bhat0???cbhat1

### Quiz 9

```{r}
fit1 <- lm(mpg ~ wt, data = mtcars)
fit2 <- lm(mpg ~ 1, data = mtcars)
num <- sum((predict(fit1)-mtcars$mpg)^2)
den <- sum((predict(fit2)-mtcars$mpg)^2)
num/den
1 - summary(fit1)$r.squared#options
```


### Quiz 10

```{r}
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
sum(resid(lm(y ~ x)))
sum(resid(lm(y ~ 1)))
sum(resid(lm(y ~ x - 1)))
#If an intercept is included, then they will sum to 0.
```


## Week 4

### Quiz 1
```{r echo=F}
data(mtcars)
SUM <- summary(lm(mpg ~
                      I(1 * (cyl == 8)) + 
                      I(1 * (cyl == 4)) +
                      wt,mtcars))$coef
pander(SUM)
SUM[2,1] - SUM[3,1]
```

### Quiz 2

```{r echo=F}
pander(summary(lm(mpg ~
                      I(1 * (cyl == 8)) + 
                      I(1 * (cyl == 4))
                  ,mtcars))$coef)
```

### Quiz 3

```{r echo=F}
lm(mpg ~  factor(cyl) + wt , data = mtcars)
lm(mpg ~  factor(cyl), data = mtcars)
```

### Quiz 4

```{r echo=F}
summary(lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars))$coef

```

函数step可以实现基于AIC准则的模型选择，最优模型为AIC值最小的

```{r echo=F}
fit <- lm(mpg ~ . + I(wt * 0.5), data = mtcars)
fit <- lm(mpg ~ .,mtcars)
pander(step(fit,trace = 0))
```


### Quiz 5

```{r }
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y ~ x)
round(hatvalues(fit),4)

```

### Quiz 6

* 异常值

- 杠杆点
    - 杠杆点对回归系数没有影响，但是会影响决定系数，可以通过观察帽子矩阵来识别，杠杆作用的平均值为h=p/n，p为自变量个数，n为样本量，如果一个观测值的杠杆值>2h，则应考虑为杠杆点，考虑剔除或采取措施。R中可以通过hatvalues函数计算杠杆值。

- 影响点
    - 影响点有将回归线拉向它的趋势，因此会影响回归系数的值，可以通过COOK距离来判断，R中可以通过cooks.distance函数计算每个观测值的COOK距离。根据经验，如果距离大于1，则说明观测点为影响点。

- 此外，还可以通过dffits函数和dfbetas函数计算相应的值，如果dffits>2/根号p/n，那么可认为是影响点，如果dfbetas>2/根号n，也可认为是影响点，p为自变量个数，n为样本量

```{r}
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y ~ x)
round(dfbetas(fit),4)
round(hatvalues(fit),4)
```




